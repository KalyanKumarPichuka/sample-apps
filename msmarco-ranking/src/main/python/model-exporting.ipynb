{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking Transformer ONNX model export\n",
    "\n",
    "This notebook demonstrates export of a three different Transformer models which we import to Vespa.ai for \n",
    "online serving. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer, BertPreTrainedModel, BertModel\n",
    "import transformers\n",
    "import torch \n",
    "from pathlib import Path\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Transformer (bi-encoder) for dense retrieval "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a wrapper model so that we can compute the mean pooling over the output inside ONNX. Almost all sentence-transformer models uses mean pooling. We also perform unit length normalization so we instead of angular we can use use raw inner dot product which speeds up nearest neighbor search.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPoolingEncoderONNX(BertPreTrainedModel):\n",
    "\n",
    "    def __init__(self,config):\n",
    "        super().__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        token_embeddings = self.bert(input_ids,attention_mask=attention_mask)[0]\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        sum_embeddings = sum_embeddings / sum_mask\n",
    "        return torch.nn.functional.normalize(sum_embeddings, p=2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using framework PyTorch: 1.7.1\n",
      "Found input input_ids with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found input token_type_ids with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found input attention_mask with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found output output_0 with shape: {0: 'batch'}\n",
      "Ensuring inputs are in correct order\n",
      "token_embeddings is not present in the generated input list.\n",
      "Generated inputs order: ['input_ids', 'attention_mask', 'token_type_ids']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:195: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n",
      "/usr/local/anaconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:1967: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert all(\n"
     ]
    }
   ],
   "source": [
    "encoder = MeanPoolingEncoderONNX.from_pretrained(\"sentence-transformers/msmarco-MiniLM-L-6-v3\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"sentence-transformers/msmarco-MiniLM-L-6-v3\")\n",
    "encoder = encoder.eval()\n",
    "pipeline = transformers.Pipeline(model=encoder, tokenizer=tokenizer)\n",
    "import transformers.convert_graph_to_onnx as onnx_convert\n",
    "onnx_convert.convert_pytorch(pipeline, opset=11, output=Path(\"sentence-msmarco-MiniLM-L-6-v3.onnx\"), use_external_format=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of onnxruntime 1.4.0, models larger than 2GB will fail to quantize due to protobuf constraint.\n",
      "This limitation will be removed in the next release of onnxruntime.\n",
      "Warning: onnxruntime.quantization.quantize is deprecated.\n",
      "         Please use quantize_static for static quantization, quantize_dynamic for dynamic quantization.\n",
      "Quantized model has been written at sentence-msmarco-MiniLM-L-6-v3-quantized.onnx: ✔\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('sentence-msmarco-MiniLM-L-6-v3-quantized.onnx')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_convert.quantize(Path(\"sentence-msmarco-MiniLM-L-6-v3.onnx\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vespa ColBERT model (Late interaction model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VespaColBERT(BertPreTrainedModel):\n",
    "\n",
    "    def __init__(self,config):\n",
    "        super().__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.linear = nn.Linear(config.hidden_size, 32, bias=False)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        Q = self.bert(input_ids,attention_mask=attention_mask)[0]\n",
    "        Q = self.linear(Q)\n",
    "        return torch.nn.functional.normalize(Q, p=2, dim=2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "colbert_query_encoder = VespaColBERT.from_pretrained(\"vespa-engine/col-minilm\") \n",
    "input_names = [\"input_ids\", \"attention_mask\"]\n",
    "output_names = [\"contextual\"]\n",
    "#input, max 32 query term\n",
    "input_ids = torch.ones(1,32, dtype=torch.int64)\n",
    "attention_mask = torch.ones(1,32,dtype=torch.int64)\n",
    "args = (input_ids, attention_mask)\n",
    "torch.onnx.export(colbert_query_encoder,\n",
    "                args=args,\n",
    "                f=\"vespa-colMiniLM-L-6.onnx\",\n",
    "                input_names = input_names,\n",
    "                output_names = output_names,\n",
    "                dynamic_axes = {\n",
    "                    \"input_ids\": {0: \"batch\"},\n",
    "                    \"attention_mask\": {0: \"batch\"},\n",
    "                    \"contextual\": {0: \"batch\"},\n",
    "                },\n",
    "                opset_version=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of onnxruntime 1.4.0, models larger than 2GB will fail to quantize due to protobuf constraint.\n",
      "This limitation will be removed in the next release of onnxruntime.\n",
      "Warning: onnxruntime.quantization.quantize is deprecated.\n",
      "         Please use quantize_static for static quantization, quantize_dynamic for dynamic quantization.\n",
      "Quantized model has been written at vespa-colMiniLM-L-6-quantized.onnx: ✔\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('vespa-colMiniLM-L-6-quantized.onnx')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_convert.quantize(Path(\"vespa-colMiniLM-L-6.onnx\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Attention Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using framework PyTorch: 1.7.1\n",
      "Found input input_ids with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found input token_type_ids with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found input attention_mask with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found output output_0 with shape: {0: 'batch'}\n",
      "Ensuring inputs are in correct order\n",
      "position_ids is not present in the generated input list.\n",
      "Generated inputs order: ['input_ids', 'attention_mask', 'token_type_ids']\n",
      "As of onnxruntime 1.4.0, models larger than 2GB will fail to quantize due to protobuf constraint.\n",
      "This limitation will be removed in the next release of onnxruntime.\n",
      "Warning: onnxruntime.quantization.quantize is deprecated.\n",
      "         Please use quantize_static for static quantization, quantize_dynamic for dynamic quantization.\n",
      "Quantized model has been written at ms-marco-MiniLM-L-6-v2-quantized.onnx: ✔\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('ms-marco-MiniLM-L-6-v2-quantized.onnx')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "cross_model = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "output_file = \"ms-marco-MiniLM-L-6-v2.onnx\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(cross_model)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(cross_model)\n",
    "model = model.eval()\n",
    "pipeline = transformers.Pipeline(model=model, tokenizer=tokenizer)\n",
    "onnx_convert.convert_pytorch(pipeline, opset=11, output=Path(output_file), use_external_format=False)\n",
    "onnx_convert.quantize(Path(output_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
