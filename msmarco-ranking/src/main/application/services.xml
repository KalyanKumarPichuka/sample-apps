<?xml version="1.0" encoding="utf-8" ?>
<services version='1.0'> 

  <container id='default' version='1.0'>
     <nodes>
      <node hostalias="node1" />
    </nodes>
    <component id="ai.vespa.tokenizer.BertTokenizer" bundle="msmarco">
      <config name="ai.vespa.tokenizer.bert-model">
        <max_input>256</max_input>
        <vocabulary>models/bert-base-uncased-vocab.txt</vocabulary>
      </config>
    </component>
    <search>
      <chain id="docranking" inherits="vespa">
        <searcher id="ai.vespa.searcher.RetrievalModelSearcher" bundle="msmarco"/>
      </chain>
      <chain id="passageranking" inherits="vespa">
        <searcher id="ai.vespa.searcher.RetrievalModelSearcher" bundle="msmarco"/>
        <searcher id="ai.vespa.searcher.colbert.ColBERTSearcher" bundle="msmarco">
          <config name="ai.vespa.colbert.colbert">
            <max_query_length>32</max_query_length>
            <dim>32</dim>
            <rank_profile>colbert_query_encoder</rank_profile>
            <output_name>onnxModel(encoder).contextual</output_name>
          </config>
        </searcher>
      </chain>
    </search>
    <document-api/>
    <document-processing/>
  </container>

  <content id='msmarco' version='1.0'>
    <redundancy>2</redundancy>
    <documents>
      <document mode='index' type='doc'/>
      <document mode='index' type='passage'/>
      <document mode='index' type='query'/>
      <document-processing cluster="default"/> 
    </documents>
    <nodes>
      <node hostalias="node1" distribution-key="0" />
    </nodes>
    <engine>
      <proton>
        <tuning>
          <searchnode>
           <requestthreads>
            <persearch>12</persearch>
           </requestthreads>
           <feeding>
            <concurrency>1.0</concurrency>
          </feeding>
          </searchnode>
        </tuning>
      </proton>
    </engine>
  </content>
</services>
